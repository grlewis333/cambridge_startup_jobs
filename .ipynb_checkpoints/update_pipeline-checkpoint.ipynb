{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b3e508",
   "metadata": {},
   "source": [
    "# Cambridge Tech Job Board — Update Notebook\n",
    "\n",
    "**Run this from the `Cambridge job site` folder.**\n",
    "\n",
    "Sections:\n",
    "1. [Quick Site Rebuild](#1.-Quick-Site-Rebuild) — regenerate HTML from existing CSV, no API needed\n",
    "2. [View & Edit Companies](#2.-View-&-Edit-Companies) — browse data, fix errors, add companies manually\n",
    "3. [Geocode New Postcodes](#3.-Geocode-New-Postcodes) — update lat/lon for any new/missing postcodes\n",
    "4. [AI Re-enrichment](#4.-AI-Re-enrichment) — re-run GPT enrichment on specific companies (needs OpenAI key)\n",
    "5. [Full Pipeline Refresh](#5.-Full-Pipeline-Refresh) — scrape hubs + Companies House + enrich everything\n",
    "\n",
    "---\n",
    "> **Architecture:** `final_companies.csv` is the master file. Edit it directly (or via cells below),\n",
    "> then run the rebuild cell to regenerate the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8094d6e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base folder: /Users/georgelewis/Documents/GitHub/cambridge_startup_jobs\n",
      "Master CSV:  True (final_companies.csv)\n",
      "Companies:   700\n",
      "Columns:     ['company_name', 'url', 'source', 'hub_name', 'hub_type', 'company_number', 'postcode', 'ch_status', 'sic_code', 'company_size', 'incorporated', 'last_accounts', 'address', 'ch_validated', 'ch_match_score', 'ch_match_name', 'ch_concern', 'has_url', 'founded_year', 'description', 'sector_tags', 'stage', 'tech_keywords', 'employee_est', 'hiring_status', 'careers_url', 'has_careers_page', 'role_count', 'roles_json', 'contact_email']\n"
     ]
    }
   ],
   "source": [
    "# ── Setup ───────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Must be run from the 'Cambridge job site' directory\n",
    "BASE = Path('.').resolve()\n",
    "MASTER_CSV    = BASE / 'pipeline/output/final_companies.csv'\n",
    "GEOCODES_A    = BASE / 'pipeline/output/geocodes_a.json'\n",
    "GEOCODES_B    = BASE / 'pipeline/output/geocodes_b.json'\n",
    "BUILD_SCRIPT  = BASE / 'build_site.py'\n",
    "HTML_SCRIPT   = BASE / 'gen_html.py'\n",
    "OUTPUT_HTML   = BASE / 'cambridge_job_board.html'\n",
    "\n",
    "print(f'Base folder: {BASE}')\n",
    "print(f'Master CSV:  {MASTER_CSV.exists()} ({MASTER_CSV.name})')\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "print(f'Companies:   {len(df)}')\n",
    "print(f'Columns:     {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4d757",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Quick Site Rebuild\n",
    "Regenerates `cambridge_job_board.html` from the existing `final_companies.csv`.  \n",
    "No API calls. Takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd42a683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebuild_site():\n",
    "    \"\"\"Regenerate the website from final_companies.csv. No API calls needed.\"\"\"\n",
    "    print('Building site data...')\n",
    "    r1 = subprocess.run([sys.executable, str(BUILD_SCRIPT)], capture_output=True, text=True, cwd=BASE)\n",
    "    if r1.returncode != 0:\n",
    "        print('ERROR in build_site.py:')\n",
    "        print(r1.stderr)\n",
    "        return\n",
    "    print(r1.stdout.strip())\n",
    "\n",
    "    print('Generating HTML...')\n",
    "    r2 = subprocess.run([sys.executable, str(HTML_SCRIPT)], capture_output=True, text=True, cwd=BASE)\n",
    "    if r2.returncode != 0:\n",
    "        print('ERROR in gen_html.py:')\n",
    "        print(r2.stderr)\n",
    "        return\n",
    "    print(r2.stdout.strip())\n",
    "    print(f'\\nDone! Open: {OUTPUT_HTML}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2367324-9048-41da-bd7d-72763df81001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building site data...\n",
      "Geocodes loaded: 126 postcodes (126 with real coords)\n",
      "Companies: 700, Roles: 28\n",
      "Sector options: 80\n",
      "Data ready.\n",
      "Saved /Users/georgelewis/Library/CloudStorage/GoogleDrive-grlewis333@gmail.com/My Drive/grl31@cam.ac.uk/Cloud Storage/Code/Cambridge job site/site_data.json\n",
      "Generating HTML...\n",
      "Written: /Users/georgelewis/Library/CloudStorage/GoogleDrive-grlewis333@gmail.com/My Drive/grl31@cam.ac.uk/Cloud Storage/Code/Cambridge job site/cambridge_job_board.html\n",
      "Size: 1103 KB\n",
      "\n",
      "Done! Open: /Users/georgelewis/Library/CloudStorage/GoogleDrive-grlewis333@gmail.com/My Drive/grl31@cam.ac.uk/Cloud Storage/Code/Cambridge job site/cambridge_job_board.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rebuild_site()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2daad6c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. View & Edit Companies\n",
    "\n",
    "### 2a. Browse the master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MASTER_CSV)\n",
    "\n",
    "# Summary stats\n",
    "print(f'Total companies: {len(df)}')\n",
    "print(f'Sources:         {df[\"source\"].value_counts().to_dict()}')\n",
    "print(f'With URL:        {df[\"url\"].notna().sum()}')\n",
    "print(f'With postcode:   {df[\"postcode\"].notna().sum()}')\n",
    "print(f'Hiring:          {df[\"hiring_status\"].value_counts().to_dict()}')\n",
    "print()\n",
    "\n",
    "# Show a searchable view of key columns\n",
    "view_cols = ['company_name', 'url', 'source', 'stage', 'hiring_status',\n",
    "             'postcode', 'founded_year', 'description']\n",
    "df[view_cols].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a366805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Search for a specific company ──────────────────────────────────────────\n",
    "SEARCH = 'echion'   # <-- change this\n",
    "\n",
    "mask = df['company_name'].str.lower().str.contains(SEARCH.lower(), na=False)\n",
    "result = df[mask][view_cols]\n",
    "print(f'Found {len(result)} match(es) for \"{SEARCH}\":')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ce013",
   "metadata": {},
   "source": [
    "### 2b. Edit an existing company\n",
    "\n",
    "Set the company name and fields to update below, then run the cell, then run **Rebuild** above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28312a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MASTER_CSV)\n",
    "\n",
    "# ── Configure edits here ───────────────────────────────────────────────────\n",
    "COMPANY_NAME = 'Echion Technologies'  # must match exactly (case-insensitive)\n",
    "\n",
    "UPDATES = {\n",
    "    # 'url':           'https://new-url.com',\n",
    "    # 'careers_url':   'https://new-url.com/careers',\n",
    "    # 'hiring_status': 'actively_hiring',   # actively_hiring | possibly_hiring | no_info\n",
    "    # 'description':   'New description text.',\n",
    "    # 'stage':         'scaleup',           # startup | scaleup | established\n",
    "    # 'postcode':      'CB22 3FG',\n",
    "    # 'founded_year':  2017,\n",
    "}\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if not UPDATES:\n",
    "    print('No updates configured — add fields to UPDATES dict above.')\n",
    "else:\n",
    "    mask = df['company_name'].str.lower() == COMPANY_NAME.lower()\n",
    "    if mask.sum() == 0:\n",
    "        print(f'ERROR: \"{COMPANY_NAME}\" not found in master CSV.')\n",
    "    else:\n",
    "        for col, val in UPDATES.items():\n",
    "            df.loc[mask, col] = val\n",
    "            print(f'  Set {col} = {val!r}')\n",
    "        df.to_csv(MASTER_CSV, index=False)\n",
    "        print(f'Saved! Now run rebuild_site() to update the website.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87443210",
   "metadata": {},
   "source": [
    "### 2c. Add a new company manually\n",
    "\n",
    "Fill in the fields below, then run the cell. The company will be appended to `final_companies.csv`.  \n",
    "Then run rebuild. Leave `description` blank if you want to auto-enrich via GPT (Section 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MASTER_CSV)\n",
    "\n",
    "# ── Fill in the new company ────────────────────────────────────────────────\n",
    "new_company = {\n",
    "    'company_name':  'Example Co',\n",
    "    'url':           'https://example.com',\n",
    "    'source':        'hub',              # hub | companies_house\n",
    "    'hub_name':      '',                 # which hub, if any\n",
    "    'postcode':      'CB1 2AB',\n",
    "    'description':   '',                 # leave blank for GPT enrichment\n",
    "    'sector_tags':   '[\"software\"]',     # JSON list, e.g. '[\"biotech\", \"AI/ML\"]'\n",
    "    'stage':         'startup',          # startup | scaleup | established\n",
    "    'employee_est':  '11-50',\n",
    "    'hiring_status': 'no_info',          # actively_hiring | possibly_hiring | no_info\n",
    "    'careers_url':   '',\n",
    "    'founded_year':  None,\n",
    "    'tech_keywords': '',\n",
    "    'ch_validated':  False,\n",
    "    'has_url':       True,\n",
    "    'has_careers_page': False,\n",
    "    'role_count':    0,\n",
    "}\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Check not already in CSV\n",
    "existing = df['company_name'].str.lower() == new_company['company_name'].lower()\n",
    "if existing.any():\n",
    "    print(f'WARNING: \"{new_company[\"company_name\"]}\" already exists. Use the edit cell instead.')\n",
    "else:\n",
    "    new_row = pd.DataFrame([new_company])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df.to_csv(MASTER_CSV, index=False)\n",
    "    print(f'Added \"{new_company[\"company_name\"]}\" — total companies: {len(df)}')\n",
    "    print('Run rebuild_site() to update the website.')\n",
    "    print()\n",
    "    print('TIP: run the GPT enrichment cell (Section 4) to auto-fill description, sector tags etc.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5b23",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Geocode New Postcodes\n",
    "\n",
    "Fetches precise lat/lon for every postcode in `final_companies.csv` using the postcodes.io batch API.  \n",
    "Results are saved to `geocodes_a.json` and used as exact pins on the map.\n",
    "\n",
    "**Requires internet access** — run this from your local machine.  \n",
    "Already-geocoded postcodes are skipped, so it's safe to re-run after adding new companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4d4c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique postcodes:  127\n",
      "Already geocoded:        64\n",
      "Need geocoding:          63\n",
      "\n",
      "Missing: ['CB2 3BZ', 'CB22 3AT', 'CB21 6DP', 'CB22 3FG', 'CB1 2JH', 'CB1 2LA', 'CB2 1PH', 'CB22 3FH', 'CB21 6GP', 'CB22 3FT', 'CB2 8EA', 'CB1 2LG', 'CB23 6DW', 'CB23 2TA', 'CB1 2JD', 'CB22 3EE', 'CB1 3JS', 'CB21 6GQ', 'CB1 9NJ', 'CB2 0AA', 'CB23 7AJ', 'CB1 7BN', 'CB2 0QQ', 'CB2 1SJ', 'CB22 3HG', 'CB2 1JP', 'CB2 1EZ', 'CB1 1BH', 'CB1 3HD', 'CB22 4QH', 'CB1 2GE', 'CB22 4PS', 'CB1 2GA', 'CB22 7GG', 'CB1 9PD', 'CB23 6NE', 'CB23 7QS', 'CB21 6DF', 'CB1 1AH', 'CB23 6JN', 'CB22 5ES', 'CB2 1GE', 'CB21 6AL', 'CB1 2FB', 'CB1 2PR', 'CB1 1HW', 'CB21 6DG', 'CB23 1ND', 'CB22 4RX', 'CB2 8BF', 'CB23 3UY', 'CB22 3FX', 'CB23 6AF', 'CB2 1ER', 'CB2 9FF', 'CB22 5LD', 'CB22 4LT', 'CB23 4RY', 'CB2 0QH', 'CB2 3QZ', 'CB22 5DU', 'CB1 7ST', 'CB21 6GB']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import httpx\n",
    "\n",
    "def load_geocodes():\n",
    "    gc = {}\n",
    "    for p in [GEOCODES_A, GEOCODES_B]:\n",
    "        if p.exists():\n",
    "            gc.update(json.load(open(p)))\n",
    "    return gc\n",
    "\n",
    "def geocode_uk_postcodes(pcs: list) -> list:\n",
    "    \"\"\"\n",
    "    Batch-geocode UK postcodes via postcodes.io (100 per request).\n",
    "    Returns list of {clean_pc, lat, lon} dicts for successful lookups.\n",
    "    \"\"\"\n",
    "    url = \"https://api.postcodes.io/postcodes\"\n",
    "    results = []\n",
    "    for i in range(0, len(pcs), 100):\n",
    "        batch = [p for p in pcs[i:i+100] if p]\n",
    "        try:\n",
    "            r = httpx.post(url, json={\"postcodes\": batch}, timeout=20.0)\n",
    "            if r.status_code == 200:\n",
    "                for item in r.json()[\"result\"]:\n",
    "                    if item[\"result\"]:\n",
    "                        results.append({\n",
    "                            \"clean_pc\": item[\"query\"],\n",
    "                            \"lat\":      item[\"result\"][\"latitude\"],\n",
    "                            \"lon\":      item[\"result\"][\"longitude\"],\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"  No result for: {item['query']}\")\n",
    "            else:\n",
    "                print(f\"  HTTP {r.status_code} on batch {i//100 + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error on batch {i//100 + 1}: {e}\")\n",
    "        time.sleep(0.1)\n",
    "    return results\n",
    "\n",
    "# ── Check what's missing ──────────────────────────────────────────────────\n",
    "df   = pd.read_csv(MASTER_CSV)\n",
    "gc   = load_geocodes()\n",
    "\n",
    "all_pcs  = df['postcode'].dropna().str.strip().str.upper().unique().tolist()\n",
    "have     = {pc for pc in all_pcs if pc in gc and gc[pc].get('lat') is not None}\n",
    "missing  = [pc for pc in all_pcs if pc not in have]\n",
    "\n",
    "print(f\"Total unique postcodes:  {len(all_pcs)}\")\n",
    "print(f\"Already geocoded:        {len(have)}\")\n",
    "print(f\"Need geocoding:          {len(missing)}\")\n",
    "if missing:\n",
    "    print(f\"\\nMissing: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a6c380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 63 postcodes from postcodes.io...\n",
      "  No result for: CB23 4RY\n",
      "\n",
      "Geocoded:  62/63\n",
      "Failed:    ['CB23 4RY']  (map will use district-level fallback for these)\n",
      "Saved to:  geocodes_a.json\n",
      "\n",
      "Rebuilding site...\n",
      "Building site data...\n",
      "Geocodes loaded: 126 postcodes (126 with real coords)\n",
      "Companies: 700, Roles: 28\n",
      "Sector options: 80\n",
      "Data ready.\n",
      "Saved /Users/georgelewis/Library/CloudStorage/GoogleDrive-grlewis333@gmail.com/My Drive/grl31@cam.ac.uk/Cloud Storage/Code/Cambridge job site/site_data.json\n",
      "Generating HTML...\n",
      "Written: /Users/georgelewis/Library/CloudStorage/GoogleDrive-grlewis333@gmail.com/My Drive/grl31@cam.ac.uk/Cloud Storage/Code/Cambridge job site/cambridge_job_board.html\n",
      "Size: 1103 KB\n",
      "\n",
      "Done! Open: /Users/georgelewis/Library/CloudStorage/GoogleDrive-grlewis333@gmail.com/My Drive/grl31@cam.ac.uk/Cloud Storage/Code/Cambridge job site/cambridge_job_board.html\n"
     ]
    }
   ],
   "source": [
    "# ── Fetch and save ────────────────────────────────────────────────────────\n",
    "if not missing:\n",
    "    print(\"Nothing to do — all postcodes already geocoded.\")\n",
    "else:\n",
    "    print(f\"Fetching {len(missing)} postcodes from postcodes.io...\")\n",
    "    results = geocode_uk_postcodes(missing)\n",
    "\n",
    "    # Load existing geocodes_a and merge in new results\n",
    "    gc_a = json.load(open(GEOCODES_A)) if GEOCODES_A.exists() else {}\n",
    "    fetched = 0\n",
    "    for r in results:\n",
    "        gc_a[r[\"clean_pc\"]] = {\"lat\": r[\"lat\"], \"lon\": r[\"lon\"]}\n",
    "        fetched += 1\n",
    "\n",
    "    with open(GEOCODES_A, \"w\") as f:\n",
    "        json.dump(gc_a, f, indent=2)\n",
    "\n",
    "    failed = [pc for pc in missing if pc not in {r[\"clean_pc\"] for r in results}]\n",
    "    print(f\"\\nGeocoded:  {fetched}/{len(missing)}\")\n",
    "    if failed:\n",
    "        print(f\"Failed:    {failed}  (map will use district-level fallback for these)\")\n",
    "    print(f\"Saved to:  {GEOCODES_A.name}\")\n",
    "\n",
    "# ── Rebuild site with updated pins ────────────────────────────────────────\n",
    "print(\"\\nRebuilding site...\")\n",
    "rebuild_site()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9d7dc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. AI Re-enrichment\n",
    "\n",
    "Uses GPT-4o mini to generate/update descriptions, sector tags, stage, hiring status etc.  \n",
    "**Requires an OpenAI API key.**\n",
    "\n",
    "Set which companies to re-enrich by name, or set `ENRICH_MISSING_ONLY = True` to only fill in companies with no description yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d98289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ── Set your OpenAI API key ────────────────────────────────────────────────\n",
    "# Option A: set as environment variable before launching Jupyter:\n",
    "#   export OPENAI_API_KEY=sk-...\n",
    "# Option B: set inline (don't commit this!):\n",
    "# os.environ['OPENAI_API_KEY'] = ''   # <-- paste your key here\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    print('ERROR: OPENAI_API_KEY not set. Set it above or in your environment.')\n",
    "else:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        print('OpenAI client ready.')\n",
    "    except ImportError:\n",
    "        print('Install openai: pip install openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519f4af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrichment functions ready.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_text(url, max_chars=4000):\n",
    "    \"\"\"Fetch a company homepage and extract readable text.\"\"\"\n",
    "    if not url or pd.isna(url):\n",
    "        return ''\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10,\n",
    "                         headers={'User-Agent': 'Mozilla/5.0 (compatible; research bot)'})\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for tag in soup(['script','style','nav','footer','header']):\n",
    "            tag.decompose()\n",
    "        text = ' '.join(soup.get_text(' ', strip=True).split())\n",
    "        return text[:max_chars]\n",
    "    except Exception as e:\n",
    "        return ''\n",
    "\n",
    "ENRICH_PROMPT = \"\"\"\n",
    "Given this company homepage text, return JSON with these fields:\n",
    "- description: 1-2 sentence factual description (no marketing fluff), max 200 chars\n",
    "- sector_tags: list of 1-4 tags from: biotech, pharma, medtech, diagnostics, genomics,\n",
    "  drug discovery, AI/ML, deep learning, computer vision, NLP, data analytics, SaaS,\n",
    "  developer tools, software, hardware, semiconductors, photonics, robotics, IoT,\n",
    "  quantum computing, space, defence, cleantech, climate, agritech, foodtech,\n",
    "  healthtech, fintech, edtech, cybersecurity, consulting, research, energy\n",
    "- stage: one of startup / scaleup / established\n",
    "- employee_est: one of 1-10 / 11-50 / 51-200 / 200-1k / 1k+ / unknown\n",
    "- hiring_status: one of actively_hiring / possibly_hiring / no_info\n",
    "- tech_keywords: comma-separated list of 3-5 technical keywords\n",
    "- careers_url: careers page URL if you can infer or find it, else empty string\n",
    "\n",
    "Return only valid JSON.\n",
    "\"\"\"\n",
    "\n",
    "def enrich_company(name, url, sic_code=''):\n",
    "    \"\"\"Enrich a single company using GPT. Returns a dict.\"\"\"\n",
    "    page_text = fetch_page_text(url)\n",
    "    context = f'Company: {name}\\nURL: {url}\\nSIC: {sic_code}\\n\\nHomepage text:\\n{page_text}'\n",
    "    if not page_text:\n",
    "        context += '\\n(Homepage not accessible — use your training knowledge.)'\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': ENRICH_PROMPT},\n",
    "                {'role': 'user', 'content': context}\n",
    "            ],\n",
    "            response_format={'type': 'json_object'},\n",
    "            max_tokens=400,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return json.loads(resp.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f'  GPT error for {name}: {e}')\n",
    "        return {}\n",
    "\n",
    "print('Enrichment functions ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configure which companies to enrich ───────────────────────────────────\n",
    "\n",
    "# Option A: specific companies by name\n",
    "COMPANIES_TO_ENRICH = [\n",
    "    # 'Example Co',\n",
    "    # 'Another Company',\n",
    "]\n",
    "\n",
    "# Option B: all companies with no description yet (flip to True)\n",
    "ENRICH_MISSING_ONLY = False\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "\n",
    "if ENRICH_MISSING_ONLY:\n",
    "    targets = df[df['description'].isna() | (df['description'] == '')].copy()\n",
    "elif COMPANIES_TO_ENRICH:\n",
    "    targets = df[df['company_name'].isin(COMPANIES_TO_ENRICH)].copy()\n",
    "else:\n",
    "    print('Configure COMPANIES_TO_ENRICH or set ENRICH_MISSING_ONLY = True')\n",
    "    targets = pd.DataFrame()\n",
    "\n",
    "print(f'Companies to enrich: {len(targets)}')\n",
    "if len(targets) > 0:\n",
    "    display(targets[['company_name','url','description']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dab7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run enrichment and save back to master CSV\n",
    "if len(targets) == 0:\n",
    "    print('No targets — configure the cell above first.')\n",
    "else:\n",
    "    df = pd.read_csv(MASTER_CSV)\n",
    "    updated = 0\n",
    "\n",
    "    for _, row in targets.iterrows():\n",
    "        name = row['company_name']\n",
    "        url  = row.get('url', '')\n",
    "        sic  = row.get('sic_code', '')\n",
    "        print(f'Enriching: {name} ({url})...')\n",
    "\n",
    "        result = enrich_company(name, str(url) if pd.notna(url) else '', str(sic) if pd.notna(sic) else '')\n",
    "\n",
    "        if result:\n",
    "            mask = df['company_name'] == name\n",
    "            field_map = {\n",
    "                'description':   'description',\n",
    "                'sector_tags':   'sector_tags',   # saved as JSON string\n",
    "                'stage':         'stage',\n",
    "                'employee_est':  'employee_est',\n",
    "                'hiring_status': 'hiring_status',\n",
    "                'tech_keywords': 'tech_keywords',\n",
    "                'careers_url':   'careers_url',\n",
    "            }\n",
    "            for gpt_key, csv_col in field_map.items():\n",
    "                if gpt_key in result and result[gpt_key]:\n",
    "                    val = result[gpt_key]\n",
    "                    if gpt_key == 'sector_tags' and isinstance(val, list):\n",
    "                        val = json.dumps(val)\n",
    "                    df.loc[mask, csv_col] = val\n",
    "            updated += 1\n",
    "            print(f'  -> {result.get(\"description\",\"\")[:80]}...')\n",
    "        else:\n",
    "            print(f'  -> No result.')\n",
    "\n",
    "        time.sleep(0.5)  # rate limit\n",
    "\n",
    "    df.to_csv(MASTER_CSV, index=False)\n",
    "    print(f'\\nUpdated {updated}/{len(targets)} companies. Saved to {MASTER_CSV.name}')\n",
    "    print('Run rebuild_site() to update the website.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412fdfb",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Full Pipeline Refresh\n",
    "\n",
    "Re-runs the full data pipeline: scrape hubs → Companies House merge → enrich → build site.  \n",
    "This takes a while (several minutes) and **requires an OpenAI API key**.\n",
    "\n",
    "Use this when you want a completely fresh update of all company data, not just a few edits.\n",
    "\n",
    "Steps:\n",
    "1. `01_merge_validate.py` — re-scrape hub company lists + re-pull CH data + fuzzy merge\n",
    "2. `02_find_careers.py` — scrape careers pages for all companies\n",
    "3. `03_enrich_companies.py` — GPT enrichment for all companies\n",
    "4. Rebuild site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_DIR = BASE / 'pipeline'\n",
    "\n",
    "# Check pipeline scripts exist\n",
    "scripts = ['01_merge_validate.py', '02_find_careers.py', '03_enrich_companies.py']\n",
    "for s in scripts:\n",
    "    p = PIPELINE_DIR / s\n",
    "    print(f'  {s}: {\"exists\" if p.exists() else \"MISSING\"}')\n",
    "\n",
    "# Check API key\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    print('\\nWARNING: OPENAI_API_KEY not set — enrichment step will fail.')\n",
    "    print('Set it in Section 4 or in your environment before running.')\n",
    "else:\n",
    "    print('\\nOpenAI key: set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b344c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run specific pipeline steps ────────────────────────────────────────────\n",
    "# Set to True for each step you want to run.\n",
    "# WARNING: This modifies pipeline/output files permanently.\n",
    "\n",
    "RUN_MERGE_VALIDATE = False   # Step 1: re-scrape hubs + CH merge\n",
    "RUN_FIND_CAREERS   = False   # Step 2: scrape careers pages\n",
    "RUN_ENRICH         = False   # Step 3: GPT enrichment for all companies\n",
    "RUN_REBUILD        = True    # Step 4: rebuild HTML (always safe to run)\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import time as _time\n",
    "\n",
    "def run_script(script_path, label):\n",
    "    print(f'\\n[{label}] Running {script_path.name}...')\n",
    "    t = _time.time()\n",
    "    r = subprocess.run([sys.executable, str(script_path)],\n",
    "                       capture_output=True, text=True, cwd=BASE,\n",
    "                       env={**os.environ})\n",
    "    elapsed = _time.time() - t\n",
    "    if r.returncode == 0:\n",
    "        print(f'  OK ({elapsed:.0f}s)')\n",
    "        if r.stdout:\n",
    "            print('  ' + r.stdout.strip().replace('\\n', '\\n  '))\n",
    "    else:\n",
    "        print(f'  FAILED ({elapsed:.0f}s)')\n",
    "        print('  STDERR:', r.stderr[-500:])\n",
    "    return r.returncode == 0\n",
    "\n",
    "if RUN_MERGE_VALIDATE:\n",
    "    run_script(PIPELINE_DIR / '01_merge_validate.py', 'Step 1')\n",
    "\n",
    "if RUN_FIND_CAREERS:\n",
    "    run_script(PIPELINE_DIR / '02_find_careers.py', 'Step 2')\n",
    "\n",
    "if RUN_ENRICH:\n",
    "    run_script(PIPELINE_DIR / '03_enrich_companies.py', 'Step 3')\n",
    "\n",
    "if RUN_REBUILD:\n",
    "    rebuild_site()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdade5d9",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Utilities\n",
    "\n",
    "Handy one-off cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99379fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show companies with no description (candidates for re-enrichment)\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "no_desc = df[df['description'].isna() | (df['description'] == '')]\n",
    "print(f'Companies with no description: {len(no_desc)}')\n",
    "no_desc[['company_name','url','source','postcode']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show companies marked as actively hiring\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "hiring = df[df['hiring_status'] == 'actively_hiring']\n",
    "print(f'Actively hiring companies: {len(hiring)}')\n",
    "hiring[['company_name','url','careers_url','stage','founded_year']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27965192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show postcode coverage\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "gc = load_geocodes()\n",
    "\n",
    "postcodes = df['postcode'].dropna().str.strip().str.upper()\n",
    "geocoded_count = sum(1 for pc in postcodes if pc in gc and gc[pc].get('lat') is not None)\n",
    "\n",
    "print(f'Companies: {len(df)}')\n",
    "print(f'With postcode: {postcodes.notna().sum()}')\n",
    "print(f'Postcode geocoded: {geocoded_count}')\n",
    "print(f'No postcode (scattered on map): {df[\"postcode\"].isna().sum()}')\n",
    "\n",
    "missing_pcs = [pc for pc in postcodes.unique() if pc not in gc or gc[pc].get('lat') is None]\n",
    "if missing_pcs:\n",
    "    print(f'\\nMissing geocodes: {missing_pcs}')\n",
    "    print('Run Section 3 to geocode these.')\n",
    "else:\n",
    "    print('\\nAll postcodes are geocoded!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
